{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "133227e6",
   "metadata": {},
   "source": [
    "### 1.\tCan you think of a few applications for a sequence-to-sequence RNN? What about a sequence-to-vector RNN? And a vector-to-sequence RNN?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c5cdf0e",
   "metadata": {},
   "source": [
    "In Sequence to Sequence Learning, RNN is trained to map an input sequence to an output sequence which is not necessarily of the same length. Applications are speech recognition, machine translation, image captioning and question answering.\n",
    "\n",
    "Applications of Recurrent Neural Networks (sequence-to-vector)(vector-to-sequence):\n",
    "* Prediction problems\n",
    "* Machine Translation\n",
    "* Speech Recognition\n",
    "* Language Modelling and Generating Text\n",
    "* Video Tagging\n",
    "* Generating Image Descriptions\n",
    "* Text Summarization\n",
    "* Call Center Analysis\n",
    "* Face detection, \n",
    "* OCR Applications as Image Recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33040a0b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a12203b0",
   "metadata": {},
   "source": [
    "### 2.\tWhy do people use encoder–decoder RNNs rather than plain sequence-to-sequence RNNs for automatic translation?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40dcbc5e",
   "metadata": {},
   "source": [
    "Attention is an extension to the architecture that addresses this limitation. It works by first providing a richer context from the encoder to the decoder and a learning mechanism where the decoder can learn where to pay attention in the richer encoding when predicting each time step in the output sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69725582",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "64d79806",
   "metadata": {},
   "source": [
    "### 3.\tHow could you combine a convolutional neural network with an RNN to classify videos?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e84a21dc",
   "metadata": {},
   "source": [
    "Taking advantage of the strengths of both CNN and RNN, the combination outperforms those individual models. Another method to combine them together is to let RNN encode the input representation and feed the outputs into CNN.\n",
    "\n",
    "The images of a video are fed to a CNN model to extract high-level features. The features are then fed to an RNN layer and the output of the RNN layer is connected to a fully connected layer to get the classification output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41bc2e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8fce9564",
   "metadata": {},
   "source": [
    "### 4.\tWhat are the advantages of building an RNN using dynamic_rnn() rather than static_rnn()?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212d6d05",
   "metadata": {},
   "source": [
    "The dynamic_rnn() function uses a while_loop() operation to run over the cell the appropriate number of times, and you can set swap_memory=True if you want it to swap the GPU’s memory to the CPU’s memory during backpropagation to avoid OOM errors. Conveniently, it also accepts a single tensor for all inputs at every time step (shape [None, n_steps, n_inputs]) and it outputs a single tensor for all outputs at every time step (shape [None, n_steps, n_neurons]); there is no need to stack, unstack, or transpose. The following code creates the same RNN as earlier using the dynamic_rnn() function. It’s so much nicer!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad5cc84a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bc085ad6",
   "metadata": {},
   "source": [
    "### 5.\tHow can you deal with variable-length input sequences? What about variable-length output sequences?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c711f38b",
   "metadata": {},
   "source": [
    "* How to deal with variable length?\n",
    "\n",
    "Approach 1: create separate batches of 1 sequence, each batch with its own length. Feed each batch to the model individually. ...\n",
    "\n",
    "Approach 2: create a fixed length batch, fill the unused tail lenght of each sequence with 0, use the parameter mask_zero=True in the embedding layers.\n",
    "\n",
    "In the case of variable length sequence prediction problems, this requires that your data be transformed such that each sequence has the same length. This vectorization allows code to efficiently perform the matrix operations in batch for your chosen deep learning algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce3e2f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e631e635",
   "metadata": {},
   "source": [
    "### 6.\tWhat is a common way to distribute training and execution of a deep RNN across multiple GPUs?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63d9ee1",
   "metadata": {},
   "source": [
    "If you have more than one GPU, the GPU with the lowest ID will be selected by default. However, TensorFlow does not place operations into multiple GPUs automatically. To override the device placement to use multiple GPUs, we manually specify the device that a computation node should run on.\n",
    "\n",
    "Distributed deep learning is one such method that enables data scientists to massively increase their productivity by (1) running parallel experiments over many devices (GPUs/TPUs/servers) and (2) massively reducing training time by distributing the training of a single network over many devices.\n",
    "\n",
    "The GPUs can all be on the same node or across multiple nodes. Only gradients are passed between the processes/GPUs. During training, each process loads its own mini-batch from disk and passes it to its GPU. Each GPU does its forward pass, then the gradients are all-reduced across the GPUs.\n",
    "\n",
    "1. Copy the model on every GPU.\n",
    "2. Split the dataset and fit the models on different subsets.\n",
    "3. Communicate the gradients at each iteration to keep the models in sync."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a801c65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
